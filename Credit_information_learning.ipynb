{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab9ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Your selected dataframe has 122 columns.\n",
      "There are 67 columns that have missing values.\n",
      "3 columns were label encoded.\n",
      "Training Features shape:  (307511, 243)\n",
      "Most Positive Correlations:\n",
      " DAYS_REGISTRATION                                    0.041975\n",
      "OCCUPATION_TYPE_Laborers                             0.043019\n",
      "FLAG_DOCUMENT_3                                      0.044346\n",
      "REG_CITY_NOT_LIVE_CITY                               0.044395\n",
      "FLAG_EMP_PHONE                                       0.045982\n",
      "NAME_EDUCATION_TYPE_Secondary / secondary special    0.049824\n",
      "REG_CITY_NOT_WORK_CITY                               0.050994\n",
      "DAYS_ID_PUBLISH                                      0.051457\n",
      "CODE_GENDER_M                                        0.054713\n",
      "DAYS_LAST_PHONE_CHANGE                               0.055218\n",
      "NAME_INCOME_TYPE_Working                             0.057481\n",
      "REGION_RATING_CLIENT                                 0.058899\n",
      "REGION_RATING_CLIENT_W_CITY                          0.060893\n",
      "DAYS_BIRTH                                           0.078239\n",
      "TARGET                                               1.000000\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "Most Negative Correlations:\n",
      " EXT_SOURCE_3                           -0.178919\n",
      "EXT_SOURCE_2                           -0.160472\n",
      "EXT_SOURCE_1                           -0.155317\n",
      "NAME_EDUCATION_TYPE_Higher education   -0.056593\n",
      "CODE_GENDER_F                          -0.054704\n",
      "NAME_INCOME_TYPE_Pensioner             -0.046209\n",
      "ORGANIZATION_TYPE_XNA                  -0.045987\n",
      "DAYS_EMPLOYED                          -0.044932\n",
      "FLOORSMAX_AVG                          -0.044003\n",
      "FLOORSMAX_MEDI                         -0.043768\n",
      "FLOORSMAX_MODE                         -0.043226\n",
      "EMERGENCYSTATE_MODE_No                 -0.042201\n",
      "HOUSETYPE_MODE_block of flats          -0.040594\n",
      "AMT_GOODS_PRICE                        -0.039645\n",
      "REGION_POPULATION_RELATIVE             -0.037227\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assignment title: Credit information learning\n",
    "# Objectives:\n",
    "# Perform machine learning on something close to real data\n",
    "# --------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#  [Problem 1] Confirmation of competition contents \n",
    "# -------------------------------------------------\n",
    "\n",
    "# What to learn and what to predict?\n",
    "# ----------------------------------\n",
    "# Learn from  a variety of alternative data--including telco and transactional information.\n",
    "# predict their clients' repayment abilities.\n",
    "\n",
    "# What kind of file to create and submit to Kaggle?\n",
    "# -------------------------------------------------\n",
    "# any source or executable code developed in connection with or based upon the Competition Data\n",
    "\n",
    "# What kind of index value will be used to evaluate the submissions?\n",
    "# ------------------------------------------------------------------\n",
    "# Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n",
    "\n",
    "\n",
    "# [Problem 2] Learning and verification\n",
    "# -------------------------------------\n",
    "app_train = pd.read_csv('application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()\n",
    "app_train['TARGET'].value_counts()\n",
    "app_train['TARGET'].astype(int).plot.hist();\n",
    "\n",
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "# Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)\n",
    "\n",
    "# Number of each type of column\n",
    "app_train.dtypes.value_counts()\n",
    "\n",
    "# Number of unique classes in each object column\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)\n",
    "\n",
    "# one-hot encoding of categorical variables\n",
    "app_train = pd.get_dummies(app_train)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "\n",
    "# Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))\n",
    "\n",
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\n",
    "\n",
    "\n",
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
    "age_data.head(10)\n",
    "\n",
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');\n",
    "\n",
    "\n",
    "# Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');\n",
    "\n",
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)\n",
    "\n",
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, diag_sharey=False,hue = 'TARGET',vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);\n",
    "\n",
    "\n",
    "# [Problem 4] Feature engineering\n",
    "# -------------------------------\n",
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "                                  \n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "# [Problem 3] Estimation on test data\n",
    "# -----------------------------------\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
